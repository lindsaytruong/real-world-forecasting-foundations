{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.8: Timeseries Diagnostics\n",
    "\n",
    "> **Goal:** Explore characteristics of the M5 dataset using tsfeatures + tsforge.\n",
    "\n",
    "This module teaches you to:\n",
    "1. Load data\n",
    "2. Compute diagnostics at the most granular \"unique_id\" level. \n",
    "3. Motivate the focus on the \"Lie Detector Six\" metric set.\n",
    "    * getting a feel for the forecastability, quality and characteristics of the data BEFORE we start forecasting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "from tsforge import load_m5\n",
    "import tsforge as tsf\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook agenda \n",
    "\n",
    "* Reconfirm the unique_id definition\n",
    "* Compute tsfeatures + tsforge diagnostics per unique_id\n",
    "* Highlight the “Lie Detector Six”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data \n",
    "weekly_df = pd.read_parquet(\n",
    "    \"/Users/jackrodenberg/Desktop/real-world-forecasting-foundations/modules/output/m5_weekly_clean.parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsforge.eda.ts_features_extension import permutation_entropy,MI_top_k_lags,ADI\n",
    "from tsfeatures import tsfeatures,lumpiness,stl_features,statistics\n",
    "# using nixtla's tsfeatures \n",
    "id_lvl_feats = tsfeatures(\n",
    "\n",
    "    ts = weekly_df,\n",
    "    # frequency of data is weekly, so here we input 52     \n",
    "    freq=52,\n",
    "\n",
    "    # COMPUTE LIE detector six \n",
    "    features=[\n",
    "        statistics,\n",
    "        lumpiness, # variance of variances \n",
    "        permutation_entropy, # permutation entropy \n",
    "        MI_top_k_lags, # sum of MI over top 5 lags \n",
    "        stl_features, # STL decomposition Features (Trend, Seasonal Strength)\n",
    "        ADI, # Avg Demand Interval\n",
    "      #  pacf_features,\n",
    "        ],\n",
    "\n",
    "        scale=False # ENSURE YOU TURN THIS OFF for accurate statistics, otherwise outputs are standard scaled for model training.. \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* taking a closter look at the table we the \"Lie Detector 6\". \n",
    "\n",
    "    - Lumpiness: Variance of Variances \n",
    "    - Entropy (Permutation Entropy)\n",
    "    - Seasonal Strength \n",
    "    - Trend Strength\n",
    "    - MI Top K Lags: Mutual Information Top K Lags (K = 5)\n",
    "        - for more clarity this is the sum of the Mutual Information of the top 5 lags from lags 1-freq\n",
    "    - ADI: Average Demand Interval (time between demands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lvl_feats[[\"unique_id\",\"lumpiness\", \"permutation_entropy\", \"seasonal_strength\", \"trend\", \"MI_top_k_lags\", \"adi\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* add a few useful descriptors to help us understand the data in a more intuitive way. \n",
    "    - how much does each item make up of the total demand\n",
    "    - where does an item rank in terms of total sales? \n",
    "    - skewness and kurtosis (understand distribution shape)\n",
    "        - kurtosis: how heavy are the tails of the distribution ? \n",
    "        - skewness: how asymmetric is the distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some additional useful descriptors\n",
    "id_lvl_feats = id_lvl_feats.assign(\n",
    "    pct_of_demand=id_lvl_feats[\"total_sum\"] / id_lvl_feats[\"total_sum\"].sum(),\n",
    ")\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "# merge with skew, kurtosis of demand!\n",
    "id_lvl_feats = id_lvl_feats.merge(\n",
    "    weekly_df.groupby(\"unique_id\").agg(\n",
    "        skew=(\"y\", \"skew\"),\n",
    "        kurtosis=(\"y\", st.kurtosis),\n",
    "    ),\n",
    "    on=\"unique_id\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lvl_feats.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_six = [\n",
    "        \"lumpiness\",\n",
    "        \"permutation_entropy\",\n",
    "        \"seasonal_strength\",\n",
    "        \"trend\",\n",
    "        \"MI_top_k_lags\",\n",
    "        \"adi\",\n",
    "    ]\n",
    "\n",
    "descriptors = ['unique_id','skew','kurtosis','pct_of_demand']\n",
    "\n",
    "cols = descriptors + ld_six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "\n",
    "def find_top_mi_lags(y, freq, top_n=3, random_state=42):\n",
    "    \"\"\"Compute MI for lags 1-freq, return top N as (lag, mi_score) arrays.\"\"\"\n",
    "    y = y.values if isinstance(y, pd.Series) else y\n",
    "    y = y[~np.isnan(y)]\n",
    "\n",
    "    if len(y) <= freq:\n",
    "        raise ValueError(f\"Length {len(y)} must be > freq {freq}\")\n",
    "\n",
    "    # Fast lag matrix using stride tricks\n",
    "    from numpy.lib.stride_tricks import as_strided\n",
    "\n",
    "    n = len(y) - freq\n",
    "    X_lags = as_strided(y, shape=(n, freq), strides=(y.itemsize, y.itemsize))[\n",
    "        :, ::-1\n",
    "    ]  # Reverse columns for lag order 1,2,3...\n",
    "\n",
    "    # Compute all MI scores at once\n",
    "    mi_scores = mutual_info_regression(X_lags, y[freq:], random_state=random_state, n_neighbors=3)\n",
    "\n",
    "    # Return top N as arrays (avoid DataFrame until final concat)\n",
    "    top_idx = np.argpartition(mi_scores, -top_n)[-top_n:]\n",
    "    top_idx = top_idx[np.argsort(mi_scores[top_idx])[::-1]]\n",
    "\n",
    "    return np.arange(1, freq + 1)[top_idx], mi_scores[top_idx]\n",
    "\n",
    "\n",
    "def _process_group(y, group_keys, freq, top_n, random_state):\n",
    "    \"\"\"Process single series, return dict or None.\"\"\"\n",
    "    if len(y) <= freq * 2:\n",
    "        return None\n",
    "    try:\n",
    "        lags, scores = find_top_mi_lags(y, freq, top_n, random_state)\n",
    "        return {\"keys\": group_keys, \"lags\": lags, \"scores\": scores}\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_top_mi_lags_batch(df, value_col, group_cols, freq, top_n=3, random_state=42, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Find top MI lags for multiple time series with parallel processing.\n",
    "\n",
    "    Returns DataFrame with group_cols + ['lag', 'mi_score', 'rank'].\n",
    "    \"\"\"\n",
    "    # Extract groups as arrays for faster processing\n",
    "    grouped = df.groupby(group_cols)[value_col].apply(np.array)\n",
    "\n",
    "    # Parallel processing\n",
    "    results = Parallel(n_jobs=n_jobs, backend=\"loky\", batch_size=\"auto\")(\n",
    "        delayed(_process_group)(y, keys, freq, top_n, random_state) for keys, y in grouped.items()\n",
    "    )\n",
    "\n",
    "    # Build final DataFrame efficiently\n",
    "    results = [r for r in results if r is not None]\n",
    "    if not results:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Vectorized DataFrame construction\n",
    "    data = []\n",
    "    for r in results:\n",
    "        keys = r[\"keys\"] if isinstance(r[\"keys\"], tuple) else (r[\"keys\"],)\n",
    "        for i, (lag, score) in enumerate(zip(r[\"lags\"], r[\"scores\"]), 1):\n",
    "            data.append((*keys, lag, score, i))\n",
    "\n",
    "    cols = group_cols + [\"lag\", \"mi_score\", \"rank\"]\n",
    "    return pd.DataFrame(data, columns=cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_lags = find_top_mi_lags_batch(weekly_df, value_col='y', group_cols=['unique_id'], freq=52, top_n=3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_lags['lag'].value_counts()[:10] # no surprise top predictive lags are 1-9, and lag 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple heuristic we can use to mark series as short or long range dependent based on MI \n",
    "def count_longrange_dependent(series):\n",
    "    ''' we say a series is long range dependent if it has more than one lag in the top 3 > 26 (6 months) '''\n",
    "    return series[series > 26].count() > 1\n",
    "\n",
    "long_range_df = most_common_lags.assign(long_range = most_common_lags.groupby(\"unique_id\")['lag'].transform(count_longrange_dependent)).query(\"long_range == True\")\n",
    "\n",
    "dep_mapping = long_range_df[['unique_id','long_range']].drop_duplicates(\"unique_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_lvl_feats = id_lvl_feats.merge(dep_mapping,on='unique_id',how='left').assign(\n",
    "    long_range = lambda df: df['long_range'].fillna(False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for detector in ld_six:\n",
    "    id_lvl_feats[f\"{detector}_flag\"] = id_lvl_feats[detector] > id_lvl_feats[detector].quantile(\n",
    "        0.75\n",
    "    )\n",
    "\n",
    "# Build labeled dataset with prominent flags + detector details\n",
    "flag_cols = [f\"{d}_flag\" for d in ld_six]\n",
    "\n",
    "id_lvl_feats_labeled = id_lvl_feats.assign(\n",
    "    # Prominent characteristic flags\n",
    "    intermittent=id_lvl_feats[\"adi\"] >= 1.34,\n",
    "    heavy_tailed=id_lvl_feats[\"kurtosis\"].abs() > 3,\n",
    "    non_zero_min=id_lvl_feats[\"min\"] > 0,\n",
    "    # Detector summary flags\n",
    "    n_flags=id_lvl_feats[flag_cols].sum(axis=1),\n",
    "    single_flag=id_lvl_feats[flag_cols].any(axis=1),\n",
    "    double_flag=lambda df: df[\"n_flags\"] >= 2,\n",
    "    # Which detectors are flagging\n",
    "    flagged_detectors=id_lvl_feats[flag_cols].apply(\n",
    "        lambda row: [ld_six[i] for i, val in enumerate(row) if val], axis=1\n",
    "    ),\n",
    "    # Compact string representation\n",
    "    flag_pattern=lambda df: df[\"flagged_detectors\"].apply(\n",
    "        lambda x: \"|\".join([d[:4].upper() for d in x]) if x else \"CLEAN\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Quick summary\n",
    "print(\"Characteristic flags:\")\n",
    "print(f\"  Intermittent: {id_lvl_feats_labeled['intermittent'].sum()}\")\n",
    "print(f\"  Heavy-tailed: {id_lvl_feats_labeled['heavy_tailed'].sum()}\")\n",
    "print(f\"  Non-zero min: {id_lvl_feats_labeled['non_zero_min'].sum()}\")\n",
    "\n",
    "print(\"\\nLie detector flags:\")\n",
    "print(f\"  Suspect (1+ detectors): {id_lvl_feats_labeled['single_flag'].sum()}\")\n",
    "print(f\"  Highly suspect (2+ detectors): {id_lvl_feats_labeled['double_flag'].sum()}\")\n",
    "\n",
    "print(\"\\nMost common flag patterns --> Clean == No Flags:\")\n",
    "print(id_lvl_feats_labeled[\"flag_pattern\"].value_counts().head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a new flag where we call out skus with long-range dependencies \n",
    "\n",
    "id_lvl_feats_labeled['new_flag_pattern'] = np.where(id_lvl_feats_labeled['long_range'] == True,\n",
    "id_lvl_feats_labeled['flag_pattern'] + '|' + 'LONG',\n",
    "id_lvl_feats_labeled['flag_pattern'])\n",
    "\n",
    "\n",
    "dmd_by_pattern = (\n",
    "    id_lvl_feats_labeled.groupby(\"new_flag_pattern\")[\"pct_of_demand\"].sum().sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "\n",
    "# answer how much of our volume is made up of each pattern\n",
    "for marker in [\"LUMP\", \"SEAS\", \"TREN\", \"ADI\", \"MI_T\", \"PERM\",\"LONG\"]:\n",
    "    if marker in [\"SEAS\", \"TREN\"]:\n",
    "        subtractor = dmd_by_pattern.filter(\n",
    "            like=\"PERM|\" + marker\n",
    "        ).sum()  # don't recognize series that have high entropy\n",
    "    else:\n",
    "        subtractor = 0\n",
    "    print(\n",
    "        f\"Percentage of Total Volume with High {marker}: {(dmd_by_pattern.filter(like=marker).sum() - subtractor)* 100:.2f}%\"\n",
    "    )  # 70% of our data has high lumpiness!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Insights: \n",
    "\n",
    "* It looks like series with High ADI (> 1.32) have high dependence on past values, this means we could see some temporal patterns in these intermittent series. \n",
    "\n",
    "* 70% of our volume is made up of lumpy timeseries! This is a big clue, we likely will need to use robust loss functions in any ML or DL approaches as the variance is highly unstable in many of our timeseries, we could also greatly benefit from variance stabilizing transformations\n",
    "\n",
    "* Variance explained by seasonality is much lower than that of trend, much of the series with significant trend/seasonality + low complexity don't make up much of the total demand \n",
    "\n",
    "* Intermittent Series (ADI) and Serial Dependence (MI_T) are forming a very small amount of total volume. In the ADI case this makes sense and is good news, meaning we can focus on the bulk of the volume with more complex series using advanced methods and likely use intermittent methods to get a 'good enough' forecast for the high ADI series... \n",
    "\n",
    "* Out of all series, long range dependence (MI_T) comprises 15% of total volume ( 2 or more lags > lag26 are considered important)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Real World Forecasting (Poetry)",
   "language": "python",
   "name": "real-world-forecasting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
