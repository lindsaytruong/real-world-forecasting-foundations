{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.14: Automating Readiness (One Script)\n",
    "\n",
    "> **Goal:** Use `tsforge` reusable readiness tool that generates a summary report.\n",
    "\n",
    "**Key Principles:**\n",
    "- Standard metrics: min history, gaps, imputation %, frequency, sparsity\n",
    "- Keep it fast and readable\n",
    "- Highlight exceptions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "**Inputs:**\n",
    "- `./output/m5_weekly_clean.parquet` ‚Äî From Module 1.9\n",
    "\n",
    "**What this module shows:**\n",
    "- `forecast_readiness_report()` function\n",
    "- `./output/readiness_report.txt`\n",
    "- `./output/readiness_summary.parquet`\n",
    "- `./output/ready_series.csv`\n",
    "\n",
    "**Data Flow:**\n",
    "```\n",
    "Module 1.13 (manual quality checks)\n",
    "    ‚Üí Module 1.14 (automated readiness) ‚Üê YOU ARE HERE\n",
    "        ‚Üí Module 1.15 (backtest planning)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `readiness_report()` in `tsforge`\n",
    "\n",
    "One function that:\n",
    "1. Takes a DataFrame\n",
    "2. Runs all quality checks\n",
    "3. Prints a readable summary\n",
    "4. Returns structured results\n",
    "5. Saves reports for audit trail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Setup complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tsforge as tsf\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "Path('./output').mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run `readiness_report()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "FORECAST READINESS REPORT\n",
      "Generated: 2025-11-24T12:53:02\n",
      "=================================================================\n",
      "\n",
      "üìä DATASET\n",
      "   Rows: 6,848,887\n",
      "   Series: 30,490\n",
      "   Date range: 2011-01-29 to 2016-06-25\n",
      "\n",
      "‚úÖ READINESS\n",
      "   Ready: 26,747 (87.7%)\n",
      "   Not ready: 3,743\n",
      "\n",
      "üìè HISTORY (min required: 52)\n",
      "   Range: 19 - 283 periods\n",
      "   Below threshold: 156\n",
      "\n",
      "üï≥Ô∏è  GAPS (max allowed: 10.0%)\n",
      "   Series with gaps: 0\n",
      "   Above threshold: 0\n",
      "\n",
      "üìâ SPARSITY (max zeros: 70.0%)\n",
      "   Mean zero %: 28.5%\n",
      "   Sparse series: 3596\n",
      "\n",
      "‚ö†Ô∏è  ISSUES\n",
      "   ‚Ä¢ 156 series have insufficient history (<52)\n",
      "   ‚Ä¢ 3596 series are too sparse (>70.0% zeros)\n",
      "\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet('./output/m5_weekly_clean.parquet')\n",
    "\n",
    "# Run readiness report\n",
    "results = tsf.forecast_readiness_report(\n",
    "    df=df,\n",
    "    id_col='unique_id',\n",
    "    date_col='ds',\n",
    "    target_col='y',\n",
    "    min_history=52,      # 1 year\n",
    "    max_gap_pct=10.0,    # Max 10% gaps\n",
    "    max_zero_pct=70.0    # Max 70% zeros\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Saved ./output/readiness_summary.parquet\n",
      "‚úì Saved ./output/ready_series.csv (26,747 series)\n",
      "‚úì Saved ./output/readiness_report.txt\n"
     ]
    }
   ],
   "source": [
    "# Save series stats\n",
    "results['series_stats'].to_parquet('./output/readiness_summary.parquet', index=False)\n",
    "print(\"‚úì Saved ./output/readiness_summary.parquet\")\n",
    "\n",
    "# Save ready series list\n",
    "pd.DataFrame({'unique_id': results['ready_series']}).to_csv('./output/ready_series.csv', index=False)\n",
    "print(f\"‚úì Saved ./output/ready_series.csv ({len(results['ready_series']):,} series)\")\n",
    "\n",
    "# Save text report\n",
    "import io, sys\n",
    "\n",
    "tsf.save_text_report(results, './output/readiness_report.txt')\n",
    "print(\"‚úì Saved ./output/readiness_report.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "\n",
    "### The Main Function\n",
    "\n",
    "```python\n",
    "results = forecast_readiness_report(\n",
    "    df=df,\n",
    "    id_col='unique_id',\n",
    "    date_col='ds',\n",
    "    target_col='y',\n",
    "    min_history=52,\n",
    "    max_gap_pct=10.0,\n",
    "    max_zero_pct=70.0\n",
    ")\n",
    "```\n",
    "\n",
    "### What It Returns\n",
    "\n",
    "| Key | Description |\n",
    "|-----|-------------|\n",
    "| `summary` | High-level metrics |\n",
    "| `series_stats` | Per-series DataFrame |\n",
    "| `issues` | List of problems found |\n",
    "| `ready_series` | IDs ready for forecasting |\n",
    "| `not_ready_series` | IDs not ready |\n",
    "\n",
    "### Standard Metrics\n",
    "\n",
    "- ‚úÖ History length (min, max, below threshold)\n",
    "- ‚úÖ Gap percentage (series with gaps, above threshold)\n",
    "- ‚úÖ Sparsity (mean zero %, sparse count)\n",
    "- ‚úÖ Overall readiness verdict\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Run before every modeling cycle**\n",
    "2. **Save reports for audit trail**\n",
    "3. **Adjust thresholds for your use case**\n",
    "4. **Filter to ready series before training**\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next\n",
    "\n",
    "**Module 1.15: Backtest Plan - Rolling-Origin & Metrics**\n",
    "- Define evaluation strategy\n",
    "- Choose horizon, cutoffs, step size\n",
    "- Select appropriate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"MODULE 1.14 COMPLETE\")\n",
    "print(\"=\" * 65)\n",
    "print(\"\\nOutputs:\")\n",
    "print(\"  ./output/readiness_summary.parquet\")\n",
    "print(\"  ./output/readiness_report.txt\")\n",
    "print(\"  ./output/ready_series.csv\")\n",
    "print(\"\\nFunctions:\")\n",
    "print(\"  forecast_readiness_report() - Main report function\")\n",
    "print(\"  get_ready_series() - Get ready IDs\")\n",
    "print(\"  get_problem_series() - Get series by issue type\")\n",
    "print(\"  filter_to_ready() - Filter DataFrame\")\n",
    "print(\"  prepare_for_modeling() - One-stop prep\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_forge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
