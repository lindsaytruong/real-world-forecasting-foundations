{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.8: Data Preparation\n",
    "\n",
    "> **Goal:** Transform first-contact data into a forecast-ready dataset: fill gaps, apply domain-appropriate imputation, and merge known-at-time calendar features.\n",
    "\n",
    "By the end of this module, you'll have a dataset that is:\n",
    "- **Continuous in time** — no missing weeks\n",
    "- **Aligned to the business week** — Walmart's Sun-Sat fiscal week\n",
    "- **Properly labeled with metadata** — department, category, state\n",
    "- **Enriched with safe features** — known-at-time calendar attributes\n",
    "\n",
    "| Step | What | Why |\n",
    "|------|------|-----|\n",
    "| 1 | Load data | Start from cleaned weekly data, diagnose gaps |\n",
    "| 2 | Fill gaps | Complete weekly timeline for every series |\n",
    "| 3 | Impute target | Apply domain-appropriate fill policy |\n",
    "| 4 | Merge calendar | Add known-at-time features (weekly aggregated) |\n",
    "| 5 | Document | Summarize all decisions and assumptions |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete | Root: real-world-forecasting-foundations | Notebook: 1.08_data_preparation | Data: /Users/lindsaytruong/forecast-academy/real-world-forecasting-foundations/data | Cache: on\n"
     ]
    }
   ],
   "source": [
    "# --- Imports ---\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from utilsforecast.preprocessing import fill_gaps\n",
    "import forecast_foundations as ff\n",
    "import tsforge as tsf\n",
    "\n",
    "# --- Notebook Settings ---\n",
    "env = ff.setup_notebook()\n",
    "\n",
    "DATA_DIR = env.DATA_DIR\n",
    "# OUTPUT_DIR = env.OUTPUT_DIR\n",
    "cache = env.cache\n",
    "output = env.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Load Data\n",
    "\n",
    "Load from Module 1.06 and understand what we're starting with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load from Module 1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded '1_06_first_contact'\n",
      "   Shape: 6,848,638 × 8\n",
      "   Report: ✓\n"
     ]
    }
   ],
   "source": [
    "weekly_sales, report_1_06 = output.load('1_06_first_contact', with_report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Rows': '6,848,638',\n",
       " 'Series': '30,490',\n",
       " 'Dates': '2011-01-23 → 2016-06-19',\n",
       " 'Frequency': 'Weekly',\n",
       " 'History': '282 weeks (5.4 yrs)',\n",
       " 'Target zeros': '24.0%'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_1_06.summary     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Save Hierarchy Data\n",
    "\n",
    "Store hierarchy columns before `fill_gaps` — we'll rejoin them after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify hierarchy columns\n",
    "hierarchy_cols = ['item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']\n",
    "hierarchy_cols = [c for c in hierarchy_cols if c in weekly_sales.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dimension table (one row per unique_id)\n",
    "hierarchy_df = weekly_sales[['unique_id'] + hierarchy_cols].drop_duplicates(subset=['unique_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>state_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>HOBBIES_1_001_CA_2</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>CA_2</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>HOBBIES_1_001_CA_3</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>CA_3</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>HOBBIES_1_001_CA_4</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>CA_4</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>HOBBIES_1_001_TX_1</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>TX_1</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              unique_id        item_id store_id    dept_id   cat_id state_id\n",
       "0    HOBBIES_1_001_CA_1  HOBBIES_1_001     CA_1  HOBBIES_1  HOBBIES       CA\n",
       "154  HOBBIES_1_001_CA_2  HOBBIES_1_001     CA_2  HOBBIES_1  HOBBIES       CA\n",
       "307  HOBBIES_1_001_CA_3  HOBBIES_1_001     CA_3  HOBBIES_1  HOBBIES       CA\n",
       "460  HOBBIES_1_001_CA_4  HOBBIES_1_001     CA_4  HOBBIES_1  HOBBIES       CA\n",
       "614  HOBBIES_1_001_TX_1  HOBBIES_1_001     TX_1  HOBBIES_1  HOBBIES       TX"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hierarchy_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "## 2. `Q3: Cadence` — Defines the Time Grid\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #2d42a7 0%, #3a2f7e 100%); color: white; padding: 12px 20px; border-radius: 8px; margin: 10px auto; max-width: 600px;\">\n",
    "<strong>Are the time intervals regular and complete?</strong><br>\n",
    "<em>Gaps break lag features and corrupt rolling calculations.</em>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check for Gaps\n",
    "\n",
    "Use `datetime_diagnostics()` from `tsforge` to determine if we need to fill gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "      <th>n_obs</th>\n",
       "      <th>span_days</th>\n",
       "      <th>inferred_freq</th>\n",
       "      <th>obs_per_year</th>\n",
       "      <th>n_gaps</th>\n",
       "      <th>pct_missing</th>\n",
       "      <th>has_duplicates</th>\n",
       "      <th>peak_month</th>\n",
       "      <th>peak_quarter</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_1</th>\n",
       "      <td>2011-01-23</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>283</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>W-SUN</td>\n",
       "      <td>52.363602</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_2</th>\n",
       "      <td>2011-01-23</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>283</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>W-SUN</td>\n",
       "      <td>52.363602</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_3</th>\n",
       "      <td>2011-01-23</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>283</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>W-SUN</td>\n",
       "      <td>52.363602</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_CA_4</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>282</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>W-SUN</td>\n",
       "      <td>52.364260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FOODS_1_001_TX_1</th>\n",
       "      <td>2011-01-30</td>\n",
       "      <td>2016-06-19</td>\n",
       "      <td>282</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>W-SUN</td>\n",
       "      <td>52.364260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 start_date   end_date  n_obs  span_days inferred_freq  \\\n",
       "unique_id                                                                \n",
       "FOODS_1_001_CA_1 2011-01-23 2016-06-19    283     1974.0         W-SUN   \n",
       "FOODS_1_001_CA_2 2011-01-23 2016-06-19    283     1974.0         W-SUN   \n",
       "FOODS_1_001_CA_3 2011-01-23 2016-06-19    283     1974.0         W-SUN   \n",
       "FOODS_1_001_CA_4 2011-01-30 2016-06-19    282     1967.0         W-SUN   \n",
       "FOODS_1_001_TX_1 2011-01-30 2016-06-19    282     1967.0         W-SUN   \n",
       "\n",
       "                  obs_per_year  n_gaps  pct_missing  has_duplicates  \\\n",
       "unique_id                                                             \n",
       "FOODS_1_001_CA_1     52.363602       0          0.0           False   \n",
       "FOODS_1_001_CA_2     52.363602       0          0.0           False   \n",
       "FOODS_1_001_CA_3     52.363602       0          0.0           False   \n",
       "FOODS_1_001_CA_4     52.364260       0          0.0           False   \n",
       "FOODS_1_001_TX_1     52.364260       0          0.0           False   \n",
       "\n",
       "                  peak_month  peak_quarter  \n",
       "unique_id                                   \n",
       "FOODS_1_001_CA_1           5             2  \n",
       "FOODS_1_001_CA_2           6             2  \n",
       "FOODS_1_001_CA_3           2             1  \n",
       "FOODS_1_001_CA_4           3             3  \n",
       "FOODS_1_001_TX_1           5             1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run datetime diagnostics\n",
    "date_table = tsf.datetime_diagnostics(\n",
    "    df=weekly_sales,\n",
    "    id_col=\"unique_id\",\n",
    "    date_col=\"ds\",\n",
    "    target_col=\"y\",\n",
    ")\n",
    "\n",
    "date_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check total gaps across all series\n",
    "(date_table['n_gaps'] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fill Gaps\n",
    "\n",
    "Run Nixtla's `fill_gaps` per `unique_id` to create a **complete weekly timeline** for every item-store pair. This inserts rows for missing dates with `y = NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'week'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/forecast-academy/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'week'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Determine week frequency from data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m freq = pd.infer_freq(\u001b[43mweekly_sales\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mweek\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.drop_duplicates().sort_values())\n\u001b[32m      3\u001b[39m freq\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/forecast-academy/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/forecast-academy/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'week'"
     ]
    }
   ],
   "source": [
    "# Determine week frequency from data\n",
    "freq = pd.infer_freq(weekly_sales['week'].drop_duplicates().sort_values())\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before fill gaps\n",
    "len(weekly_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_filled = fill_gaps(\n",
    "    weekly_sales[['unique_id', 'ds', 'y']],\n",
    "    freq=freq\n",
    ")\n",
    "\n",
    "len(weekly_sales_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_filled.sort_values(['unique_id', 'ds']).reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag gap rows BEFORE imputation\n",
    "weekly_sales_filled['is_gap'] = weekly_sales_filled['y'].isna().astype(int)\n",
    "n_gaps = weekly_sales_filled['is_gap'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of gaps to impute\n",
    "n_gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Restore Metadata\n",
    "\n",
    "When `fill_gaps` creates new weeks, metadata columns are empty. Re-attach using our stored dimension table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejoin hierarchy\n",
    "weekly_sales_filled = weekly_sales_filled.merge(\n",
    "    hierarchy_df,\n",
    "    on='unique_id',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "## 3. `Q1: Target` — Defines What We're Predicting\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #2596be 0%, #3a2f7e 100%); color: white; padding: 12px 20px; border-radius: 8px; margin: 10px auto; max-width: 600px;\">\n",
    "<strong>How do we treat missing target values?</strong><br>\n",
    "<em>Imputation strategy depends on domain knowledge and business context.</em>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "Now that we have a complete time grid with `y = NaN` for gap rows, we need to decide **how to fill those NaNs**. This is a business decision, not a technical one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Imputation Strategies\n",
    "\n",
    "| Strategy | How It Works | When to Use |\n",
    "|----------|--------------|-------------|\n",
    "| **Zero fill** | `y = 0` | Retail: store open, no sales recorded = no sales happened |\n",
    "| **Forward fill** | Use last known value | Sensors: missing reading likely similar to previous |\n",
    "| **Backward fill** | Use next known value | Late-arriving data: value exists, just delayed |\n",
    "| **Interpolation** | Linear/spline between known points | Continuous processes: temperature, stock prices |\n",
    "| **Seasonal fill** | Use same period last year/cycle | Strong seasonality: tourism, agriculture |\n",
    "| **Mean/Median fill** | Use series average | When no pattern exists, need a neutral value |\n",
    "| **Model-based** | Predict missing values | When you have reliable exogenous features |\n",
    "| **Leave as NaN** | Don't impute | When downstream model handles missingness |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 M5 Fill Policy\n",
    "\n",
    "For M5 retail data, we apply:\n",
    "\n",
    "| Scenario | Data Pattern | Policy |\n",
    "|----------|--------------|--------|\n",
    "| Store open, item available, no one bought it | Gap in middle of active series | `y = 0` |\n",
    "| Item newly stocked, slow start | Gap at series start | `y = 0` |\n",
    "| Item still active, just no sales that week | Gap at series end | `y = 0` |\n",
    "| Store closed (holiday, renovation) | Could exclude from training or keep as 0 | Flag separately |\n",
    "| Item discontinued | Zeros followed by series end | `y = 0` until discontinuation |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fill policy: missing weeks → 0 (retail assumption: no sale = 0 units)\n",
    "weekly_sales_filled['y'] = weekly_sales_filled['y'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify no NaNs remain\n",
    "n_gaps = weekly_sales_filled['y'].isna().sum()\n",
    "\n",
    "n_gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_filled.drop(columns=['is_gap'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "## 4. `Q4: Data` — Defines What the Model Learns\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #1d1f56 0%, #2d42a7 100%); color: white; padding: 12px 20px; border-radius: 8px; margin: 10px auto; max-width: 600px;\">\n",
    "<strong>What features can we safely add without leakage?</strong><br>\n",
    "<em>Calendar features are known-at-time — safe for any forecast date.</em>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "\n",
    "The `calendar.csv` file is **daily**, but our sales are **weekly**. We need to:\n",
    "1. Determine week alignment (start vs end)\n",
    "2. Create a matching week column in calendar\n",
    "3. Aggregate daily features to weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Determine Week Alignment\n",
    "\n",
    "Our `ds` column contains Sundays (e.g., 2011-01-23). \n",
    "\n",
    "**Walmart's fiscal week runs Sunday to Saturday**, so Sunday is the **week start**:\n",
    "- `ds = 2011-01-23` represents sales from Sun 2011-01-23 through Sat 2011-01-29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create Week Column in Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calendar\n",
    "calendar = load_m5_calendar(DATA_DIR)\n",
    "calendar['date'] = pd.to_datetime(calendar['date'])\n",
    "\n",
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Walmart's fiscal week runs Sunday to Saturday**, so Sunday is the **week start\n",
    "calendar['week_start'] = calendar['date'] - pd.to_timedelta((calendar['date'].dt.dayofweek + 1) % 7, unit='D')\n",
    "calendar[['date', 'week_start', 'weekday']].head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Aggregate Calendar to Weekly\n",
    "\n",
    "| Feature | Aggregation Rule |\n",
    "|---------|------------------|\n",
    "| Holiday/Event | If any day in week has it → week has it |\n",
    "| SNAP flags | Same — max within week |\n",
    "| wm_yr_wk | From **first day of week** (Sunday) |\n",
    "| Year, month | From **first day of week** (Sunday) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `aggregate_calendar_to_weekly()` utility function to aggregate daily calendar data to weekly.\n",
    "\n",
    "This function handles:\n",
    "1. Groups daily rows by week (Sunday = week start for Walmart)\n",
    "2. Takes first value for calendar IDs (wm_yr_wk, month, year)  \n",
    "3. Collects all unique events and splits them into separate columns\n",
    "4. Uses max() for SNAP flags (1 if ANY day in week had benefits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate daily calendar to weekly using our utility function\n",
    "# The result has one row per week with event_name_1, event_name_2, etc.\n",
    "# based on however many events occurred in the week\n",
    "\n",
    "weekly_calendar = aggregate_calendar_to_weekly(calendar)\n",
    "\n",
    "weekly_calendar.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Merge into Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_sales_filled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = weekly_sales_filled.merge(\n",
    "    weekly_calendar,\n",
    "    on='ds',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final column check\n",
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 What We're NOT Adding (Yet)\n",
    "\n",
    "| Feature | Why Excluded | When to Add |\n",
    "|---------|--------------|-------------|\n",
    "| Price features | Requires lagging to avoid leakage | Feature Engineering module |\n",
    "| Lag features | Created during model training | Modeling module |\n",
    "| Outlier flags | Need baseline forecast first | Post-baseline module |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "## 5. `Q5: Ownership` — Defines Transparency\n",
    "\n",
    "<div style=\"background: linear-gradient(135deg, #1d1f56 0%, #3a2f7e 100%); color: white; padding: 12px 20px; border-radius: 8px; margin: 10px auto; max-width: 600px;\">\n",
    "<strong>What assumptions are baked into this data?</strong><br>\n",
    "<em>Document decisions so downstream users can trace and adjust.</em>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 All Decisions Made - What We Did\n",
    "\n",
    "| Step | Decision | Assumption | Reversible? |\n",
    "|------|----------|------------|-------------|\n",
    "| Gap detection | Used `datetime_diagnostics` | Weekly frequency is correct | ✓ Re-run with different freq |\n",
    "| Gap filling | Nixtla `fill_gaps` | Series should span full date range | ✓ `is_gap` flag preserved |\n",
    "| Imputation | Zero fill for all gaps | Missing = no sales (retail) | ✓ Can re-impute using `is_gap` |\n",
    "| Calendar aggregation | Events: any in week | One event day = event week | ✓ Raw calendar available |\n",
    "| Calendar aggregation | SNAP: max in week | One SNAP day = SNAP week | ✓ Raw calendar available |\n",
    "| Calendar aggregation | Fiscal: first day of week | Week inherits Sunday's attributes | ✓ Can change to last/mode |\n",
    "| Hierarchy | Static per unique_id | Items don't change department | — |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 What Changed - The Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_1_08 = first_contact_check(\n",
    "    merged_df,\n",
    "    dataset_name='Merged Weekly Sales and Calendar',\n",
    "    prior_report=report_1_06\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_1_08.changes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache for downstream modules\n",
    "artifacts.save(\n",
    "    df=merged_df,\n",
    "    report=report_1_08\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "| Module | Focus |\n",
    "|--------|-------|\n",
    "| **1.11** | Plotting & visual diagnostics |\n",
    "| **2.1** | Baseline models — naive, seasonal naive |\n",
    "| **2.2** | Statistical models — ETS, ARIMA, Theta |\n",
    "| **2.3** | Feature engineering — price features with proper lagging |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast-academy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
